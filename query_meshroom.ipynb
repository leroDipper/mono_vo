{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1083b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.feature_extractor import SIFT\n",
    "\n",
    "# --- Load dataset info ---\n",
    "with open('office_dataset_aruco/ground_truth_poses.json', 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "camera_intrinsics = ground_truth[\"camera_intrinsics\"]\n",
    "K = np.array([[camera_intrinsics[\"fx\"], 0, camera_intrinsics[\"cx\"]],\n",
    "              [0, camera_intrinsics[\"fy\"], camera_intrinsics[\"cy\"]],\n",
    "              [0, 0, 1]], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3c6d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshroom data structure:\n",
      "Keys: dict_keys(['version', 'featuresFolders', 'matchesFolders', 'views', 'intrinsics', 'poses'])\n",
      "Number of views: 121\n",
      "First view keys: dict_keys(['viewId', 'poseId', 'frameId', 'intrinsicId', 'resectionId', 'path', 'width', 'height', 'metadata'])\n",
      "Number of poses: 121\n",
      "Number of intrinsics: 1\n"
     ]
    }
   ],
   "source": [
    "# Load Meshroom's reconstruction\n",
    "with open('/home/leroy-marewangepo/Masters_Stuff/mono_vo/StructureFromMotion/output/cameras.sfm', 'r') as f:\n",
    "    meshroom_data = json.load(f)\n",
    "\n",
    "print(\"Meshroom data structure:\")\n",
    "print(f\"Keys: {meshroom_data.keys()}\")\n",
    "\n",
    "# Explore the structure\n",
    "if 'views' in meshroom_data:\n",
    "    print(f\"Number of views: {len(meshroom_data['views'])}\")\n",
    "    print(f\"First view keys: {meshroom_data['views'][0].keys() if meshroom_data['views'] else 'None'}\")\n",
    "\n",
    "if 'poses' in meshroom_data:\n",
    "    print(f\"Number of poses: {len(meshroom_data['poses'])}\")\n",
    "    \n",
    "if 'intrinsics' in meshroom_data:\n",
    "    print(f\"Number of intrinsics: {len(meshroom_data['intrinsics'])}\")\n",
    "\n",
    "if 'structure' in meshroom_data:\n",
    "    print(f\"Number of 3D points: {len(meshroom_data['structure'])}\")\n",
    "    print(f\"First 3D point keys: {meshroom_data['structure'][0].keys() if meshroom_data['structure'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75715208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Meshroom poses:\n",
      "  Frame 1: position = [ -1.32500464 -14.09570895   0.55971041]\n",
      "  Frame 5: position = [ -1.89106643 -14.03397593   0.48451178]\n",
      "  Frame 10: position = [ -0.89950241 -14.06218714  -0.15353726]\n",
      "  Frame 15: position = [ -0.540854   -14.04679548  -0.4511625 ]\n",
      "  Frame 20: position = [ -0.11677747 -14.03355539  -0.65342604]\n",
      "  Frame 25: position = [ -0.44401711 -14.03072321  -0.47325309]\n",
      "\n",
      "Found 6 out of 6 frames\n"
     ]
    }
   ],
   "source": [
    "# We want frames 1, 5, 10, 15, 20 for map, frame 25 for query\n",
    "frames_we_need = [1, 5, 10, 15, 20, 25]\n",
    "\n",
    "# Build mapping from frame number to pose\n",
    "frame_to_pose = {}\n",
    "\n",
    "for view in meshroom_data['views']:\n",
    "    frame_id = int(view['frameId'])\n",
    "    pose_id = view['poseId']\n",
    "    \n",
    "    if frame_id in frames_we_need:\n",
    "        # Find the pose with this ID\n",
    "        for pose in meshroom_data['poses']:\n",
    "            if pose['poseId'] == pose_id:\n",
    "                # Extract rotation matrix (9 elements -> 3x3)\n",
    "                rot_flat = [float(x) for x in pose['pose']['transform']['rotation']]\n",
    "                R = np.array(rot_flat).reshape(3, 3)\n",
    "                \n",
    "                # Extract camera center\n",
    "                center = np.array([float(x) for x in pose['pose']['transform']['center']])\n",
    "                \n",
    "                frame_to_pose[frame_id] = {\n",
    "                    'R': R,  # Camera-to-world rotation\n",
    "                    'center': center,  # Camera position in world\n",
    "                    'pose_id': pose_id\n",
    "                }\n",
    "                break\n",
    "\n",
    "print(\"Extracted Meshroom poses:\")\n",
    "for frame_id in sorted(frame_to_pose.keys()):\n",
    "    center = frame_to_pose[frame_id]['center']\n",
    "    print(f\"  Frame {frame_id}: position = {center}\")\n",
    "\n",
    "print(f\"\\nFound {len(frame_to_pose)} out of {len(frames_we_need)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4f3b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: 1601 keypoints\n",
      "Frame 5: 2000 keypoints\n",
      "Frame 10: 1437 keypoints\n",
      "Frame 15: 1491 keypoints\n",
      "Frame 20: 1233 keypoints\n",
      "Frame 25: 1194 keypoints\n"
     ]
    }
   ],
   "source": [
    "# We have Meshroom's camera poses in a unified coordinate system\n",
    "# Now let's triangulate using those poses instead of estimating them\n",
    "\n",
    "# Load the images we extracted features from earlier\n",
    "import os\n",
    "\n",
    "images_path = \"office_dataset_aruco/left\"\n",
    "\n",
    "def load_image(frame_num):\n",
    "    filename = f\"frame_{frame_num:04d}.png\"\n",
    "    filepath = os.path.join(images_path, filename)\n",
    "    img = cv2.imread(filepath)\n",
    "    if img is None:\n",
    "        return None\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "from modules.feature_extractor import SIFT\n",
    "\n",
    "map_frames = [1, 5, 10, 15, 20]\n",
    "query_frame = 25\n",
    "\n",
    "# Extract features\n",
    "extractor = SIFT(n_features=2000)\n",
    "features = {}\n",
    "\n",
    "for frame_num in map_frames + [query_frame]:\n",
    "    img = load_image(frame_num)\n",
    "    if img is not None:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        kp, desc = extractor.detect_and_compute(gray)\n",
    "        features[frame_num] = {\n",
    "            'keypoints': kp,\n",
    "            'descriptors': desc,\n",
    "            'image': img\n",
    "        }\n",
    "        print(f\"Frame {frame_num}: {len(kp)} keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932a0e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pair 1-5 ===\n",
      "  202 filtered matches\n",
      "  31 RANSAC inliers\n",
      "\n",
      "=== Pair 1-10 ===\n",
      "  100 filtered matches\n",
      "  55 RANSAC inliers\n",
      "\n",
      "=== Pair 5-10 ===\n",
      "  34 filtered matches\n",
      "  23 RANSAC inliers\n",
      "\n",
      "=== Pair 10-15 ===\n",
      "  182 filtered matches\n",
      "  124 RANSAC inliers\n",
      "\n",
      "=== Pair 15-20 ===\n",
      "  186 filtered matches\n",
      "  64 RANSAC inliers\n",
      "\n",
      "Processed 5 pairs successfully\n"
     ]
    }
   ],
   "source": [
    "# Match pairs using OpenCV's findFundamentalMat with RANSAC\n",
    "frame_pairs = [(1, 5), (1, 10), (5, 10), (10, 15), (15, 20)]\n",
    "\n",
    "all_pair_results = {}\n",
    "\n",
    "for frame_a, frame_b in frame_pairs:\n",
    "    print(f\"\\n=== Pair {frame_a}-{frame_b} ===\")\n",
    "    \n",
    "    # Match features\n",
    "    desc_a = features[frame_a]['descriptors']\n",
    "    desc_b = features[frame_b]['descriptors']\n",
    "    matches = extractor.match_features(desc_a, desc_b)\n",
    "    \n",
    "    # Extract matched points\n",
    "    pts_a = []\n",
    "    pts_b = []\n",
    "    match_info = []\n",
    "    \n",
    "    for match in matches:\n",
    "        pt_a = features[frame_a]['keypoints'][match.queryIdx].pt\n",
    "        pt_b = features[frame_b]['keypoints'][match.trainIdx].pt\n",
    "        \n",
    "        # Distance filter\n",
    "        if np.linalg.norm(np.array(pt_b) - np.array(pt_a)) <= 700:\n",
    "            pts_a.append(pt_a)\n",
    "            pts_b.append(pt_b)\n",
    "            match_info.append({\n",
    "                'kp_idx_a': match.queryIdx,\n",
    "                'kp_idx_b': match.trainIdx\n",
    "            })\n",
    "    \n",
    "    pts_a = np.array(pts_a)\n",
    "    pts_b = np.array(pts_b)\n",
    "    \n",
    "    print(f\"  {len(pts_a)} filtered matches\")\n",
    "    \n",
    "    # OpenCV RANSAC\n",
    "    F, mask = cv2.findFundamentalMat(pts_a, pts_b, cv2.FM_RANSAC, 3.0, 0.99)\n",
    "    \n",
    "    if mask is not None:\n",
    "        inlier_mask = mask.ravel() == 1\n",
    "        inlier_pts_a = pts_a[inlier_mask]\n",
    "        inlier_pts_b = pts_b[inlier_mask]\n",
    "        inlier_indices = np.where(inlier_mask)[0]\n",
    "        \n",
    "        print(f\"  {len(inlier_pts_a)} RANSAC inliers\")\n",
    "        \n",
    "        all_pair_results[f\"{frame_a}-{frame_b}\"] = {\n",
    "            'pts_a': inlier_pts_a,\n",
    "            'pts_b': inlier_pts_b,\n",
    "            'match_info': [match_info[i] for i in inlier_indices],\n",
    "            'frame_a': frame_a,\n",
    "            'frame_b': frame_b\n",
    "        }\n",
    "    else:\n",
    "        print(\"  RANSAC failed\")\n",
    "\n",
    "print(f\"\\nProcessed {len(all_pair_results)} pairs successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "606234ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Triangulating 1-5\n",
      "  0 valid points\n",
      "\n",
      "Triangulating 1-10\n",
      "  49 valid points\n",
      "\n",
      "Triangulating 5-10\n",
      "  6 valid points\n",
      "\n",
      "Triangulating 10-15\n",
      "  93 valid points\n",
      "\n",
      "Triangulating 15-20\n",
      "  48 valid points\n",
      "\n",
      "Total 3D points: 196 (all in Meshroom's unified coordinate system)\n"
     ]
    }
   ],
   "source": [
    "# Build projection matrices from Meshroom poses\n",
    "def build_P_from_meshroom(frame_id, K):\n",
    "    pose = frame_to_pose[frame_id]\n",
    "    R_c2w = pose['R']  # Camera-to-world\n",
    "    center = pose['center']  # Camera position in world\n",
    "    \n",
    "    # Convert to world-to-camera for projection matrix\n",
    "    R_w2c = R_c2w.T\n",
    "    t_w2c = -R_w2c @ center\n",
    "    \n",
    "    return K @ np.hstack([R_w2c, t_w2c.reshape(-1, 1)])\n",
    "\n",
    "# Triangulate all pairs using Meshroom poses\n",
    "all_triangulated = {}\n",
    "\n",
    "for pair_name, result in all_pair_results.items():\n",
    "    print(f\"\\nTriangulating {pair_name}\")\n",
    "    \n",
    "    frame_a = result['frame_a']\n",
    "    frame_b = result['frame_b']\n",
    "    \n",
    "    # Build projection matrices from Meshroom\n",
    "    P_a = build_P_from_meshroom(frame_a, K)\n",
    "    P_b = build_P_from_meshroom(frame_b, K)\n",
    "    \n",
    "    pts_a_2d = result['pts_a']\n",
    "    pts_b_2d = result['pts_b']\n",
    "    \n",
    "    # Triangulate\n",
    "    points_4d = cv2.triangulatePoints(P_a, P_b, pts_a_2d.T, pts_b_2d.T)\n",
    "    points_3d = points_4d[:3] / points_4d[3]\n",
    "    points_3d = points_3d.T\n",
    "    \n",
    "    # Check reprojection error and positive depth\n",
    "    valid_points = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, pt in enumerate(points_3d):\n",
    "        pt_hom = np.append(pt, 1)\n",
    "        \n",
    "        depth_a = (P_a @ pt_hom)[2]\n",
    "        depth_b = (P_b @ pt_hom)[2]\n",
    "        \n",
    "        if depth_a <= 0 or depth_b <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Check reprojection\n",
    "        proj_a = P_a @ pt_hom\n",
    "        proj_a_2d = proj_a[:2] / proj_a[2]\n",
    "        error_a = np.linalg.norm(proj_a_2d - pts_a_2d[i])\n",
    "        \n",
    "        proj_b = P_b @ pt_hom\n",
    "        proj_b_2d = proj_b[:2] / proj_b[2]\n",
    "        error_b = np.linalg.norm(proj_b_2d - pts_b_2d[i])\n",
    "        \n",
    "        if error_a < 5.0 and error_b < 5.0:\n",
    "            valid_points.append(pt)\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    all_triangulated[pair_name] = {\n",
    "        'points_3d': np.array(valid_points),\n",
    "        'valid_indices': valid_indices,\n",
    "        'match_info': result['match_info'],\n",
    "        'frame_a': frame_a\n",
    "    }\n",
    "    \n",
    "    print(f\"  {len(valid_points)} valid points\")\n",
    "\n",
    "total = sum(len(d['points_3d']) for d in all_triangulated.values())\n",
    "print(f\"\\nTotal 3D points: {total} (all in Meshroom's unified coordinate system)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781c3a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature database: 196 entries\n",
      "Found 295 raw matches\n",
      "Unique correspondences: 19\n",
      "\n",
      "Estimated frame 25 position: [ -4.1529214  -14.69349622  -0.09176367]\n",
      "Meshroom frame 25 position:  [ -0.44401711 -14.03072321  -0.47325309]\n",
      "Error: 3.7869 meters\n"
     ]
    }
   ],
   "source": [
    "# Build feature database\n",
    "feature_database = []\n",
    "\n",
    "for pair_name, data in all_triangulated.items():\n",
    "    frame_a = data['frame_a']\n",
    "    points_3d = data['points_3d']\n",
    "    valid_indices = data['valid_indices']\n",
    "    match_info = data['match_info']\n",
    "    \n",
    "    for i, valid_idx in enumerate(valid_indices):\n",
    "        # Get the match info for this valid point\n",
    "        match = match_info[valid_idx]\n",
    "        kp_idx_a = match['kp_idx_a']\n",
    "        descriptor = features[frame_a]['descriptors'][kp_idx_a]\n",
    "        \n",
    "        feature_database.append({\n",
    "            'descriptor': descriptor,\n",
    "            'point_3d': points_3d[i]\n",
    "        })\n",
    "\n",
    "print(f\"Feature database: {len(feature_database)} entries\")\n",
    "\n",
    "# Query with frame 25\n",
    "query_desc = features[25]['descriptors']\n",
    "query_kp = features[25]['keypoints']\n",
    "\n",
    "db_desc = np.array([e['descriptor'] for e in feature_database])\n",
    "matches = extractor.match_features(query_desc, db_desc)\n",
    "\n",
    "print(f\"Found {len(matches)} raw matches\")\n",
    "\n",
    "# Filter duplicates\n",
    "best_matches = {}\n",
    "for match in matches:\n",
    "    query_pt = query_kp[match.queryIdx].pt\n",
    "    world_pt = feature_database[match.trainIdx]['point_3d']\n",
    "    key = tuple(world_pt)\n",
    "    \n",
    "    if key not in best_matches or match.distance < best_matches[key]['distance']:\n",
    "        best_matches[key] = {'query_2d': query_pt, 'world_3d': world_pt, 'distance': match.distance}\n",
    "\n",
    "query_2d = np.array([v['query_2d'] for v in best_matches.values()])\n",
    "world_3d = np.array([v['world_3d'] for v in best_matches.values()])\n",
    "\n",
    "print(f\"Unique correspondences: {len(query_2d)}\")\n",
    "\n",
    "# Solve PnP\n",
    "if len(query_2d) >= 4:\n",
    "    success, rvec, tvec = cv2.solvePnP(world_3d, query_2d, K, None, flags=cv2.SOLVEPNP_ITERATIVE)\n",
    "    \n",
    "    if success:\n",
    "        R_est, _ = cv2.Rodrigues(rvec)\n",
    "        # PnP gives world-to-camera, we want camera position\n",
    "        cam_center_est = -R_est.T @ tvec.flatten()\n",
    "        \n",
    "        meshroom_pos = frame_to_pose[25]['center']\n",
    "        error = np.linalg.norm(cam_center_est - meshroom_pos)\n",
    "        \n",
    "        print(f\"\\nEstimated frame 25 position: {cam_center_est}\")\n",
    "        print(f\"Meshroom frame 25 position:  {meshroom_pos}\")\n",
    "        print(f\"Error: {error:.4f} meters\")\n",
    "    else:\n",
    "        print(\"PnP failed\")\n",
    "else:\n",
    "    print(f\"Not enough correspondences: {len(query_2d)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfeat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
