{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d0619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded frame 1: (960, 1280, 3)\n",
      "Loaded frame 5: (960, 1280, 3)\n",
      "Loaded frame 10: (960, 1280, 3)\n",
      "Loaded frame 15: (960, 1280, 3)\n",
      "Loaded frame 20: (960, 1280, 3)\n",
      "Loaded frame 25: (960, 1280, 3)\n",
      "Frame 1: 1601 keypoints\n",
      "Frame 5: 2000 keypoints\n",
      "Frame 10: 1437 keypoints\n",
      "Frame 15: 1491 keypoints\n",
      "Frame 20: 1233 keypoints\n",
      "Frame 25: 1194 keypoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.feature_extractor import SIFT\n",
    "\n",
    "# Load dataset\n",
    "with open('office_dataset_aruco/ground_truth_poses.json', 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "camera_intrinsics = ground_truth[\"camera_intrinsics\"]\n",
    "K = np.array([[camera_intrinsics[\"fx\"], 0, camera_intrinsics[\"cx\"]],\n",
    "              [0, camera_intrinsics[\"fy\"], camera_intrinsics[\"cy\"]],\n",
    "              [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "images_path = \"office_dataset_aruco/left\"\n",
    "\n",
    "def load_image(frame_num):\n",
    "    filename = f\"frame_{frame_num:04d}.png\"\n",
    "    filepath = os.path.join(images_path, filename)\n",
    "    img = cv2.imread(filepath)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load {filepath}\")\n",
    "        return None\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load frames for map building + query\n",
    "map_frames = [1, 5, 10, 15, 20]\n",
    "query_frame = 25\n",
    "\n",
    "frames = {}\n",
    "for frame_num in map_frames + [query_frame]:\n",
    "    img = load_image(frame_num)\n",
    "    if img is not None:\n",
    "        frames[frame_num] = img\n",
    "        print(f\"Loaded frame {frame_num}: {img.shape}\")\n",
    "\n",
    "# Extract features\n",
    "extractor = SIFT(n_features=2000)\n",
    "features = {}\n",
    "\n",
    "for frame_num, img in frames.items():\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    kp, desc = extractor.detect_and_compute(gray)\n",
    "    features[frame_num] = {\n",
    "        'keypoints': kp,\n",
    "        'descriptors': desc,\n",
    "        'image': img\n",
    "    }\n",
    "    print(f\"Frame {frame_num}: {len(kp)} keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc80ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing pair 1-5\n",
      "  288 raw matches\n",
      "  202 after distance filter\n",
      "\n",
      "Processing pair 1-10\n",
      "  122 raw matches\n",
      "  100 after distance filter\n",
      "\n",
      "Processing pair 5-10\n",
      "  35 raw matches\n",
      "  34 after distance filter\n",
      "\n",
      "Processing pair 5-15\n",
      "  15 raw matches\n",
      "  12 after distance filter\n",
      "\n",
      "Processing pair 10-15\n",
      "  189 raw matches\n",
      "  182 after distance filter\n",
      "\n",
      "Processing pair 10-20\n",
      "  121 raw matches\n",
      "  96 after distance filter\n",
      "\n",
      "Processing pair 15-20\n",
      "  206 raw matches\n",
      "  186 after distance filter\n",
      "\n",
      "Total pairs processed: 7\n"
     ]
    }
   ],
   "source": [
    "# Match all pairs of map frames to build a richer database\n",
    "# Pairs: (1,5), (1,10), (5,10), (5,15), (10,15), (10,20), (15,20)\n",
    "\n",
    "frame_pairs = [\n",
    "    (1, 5),\n",
    "    (1, 10),\n",
    "    (5, 10),\n",
    "    (5, 15),\n",
    "    (10, 15),\n",
    "    (10, 20),\n",
    "    (15, 20)\n",
    "]\n",
    "\n",
    "all_pair_data = {}\n",
    "\n",
    "for frame_a, frame_b in frame_pairs:\n",
    "    print(f\"\\nProcessing pair {frame_a}-{frame_b}\")\n",
    "    \n",
    "    # Match\n",
    "    desc_a = features[frame_a]['descriptors']\n",
    "    desc_b = features[frame_b]['descriptors']\n",
    "    matches = extractor.match_features(desc_a, desc_b)\n",
    "    \n",
    "    # Store match data\n",
    "    match_data = []\n",
    "    for match in matches:\n",
    "        pt_a = features[frame_a]['keypoints'][match.queryIdx].pt\n",
    "        pt_b = features[frame_b]['keypoints'][match.trainIdx].pt\n",
    "        \n",
    "        match_data.append({\n",
    "            'kp_idx_a': match.queryIdx,\n",
    "            'kp_idx_b': match.trainIdx,\n",
    "            'pt_a': np.array(pt_a),\n",
    "            'pt_b': np.array(pt_b),\n",
    "            'distance': match.distance\n",
    "        })\n",
    "    \n",
    "    print(f\"  {len(match_data)} raw matches\")\n",
    "    \n",
    "    # Distance filter\n",
    "    filtered = []\n",
    "    for m in match_data:\n",
    "        dist = np.linalg.norm(m['pt_b'] - m['pt_a'])\n",
    "        if dist <= 700:\n",
    "            filtered.append(m)\n",
    "    \n",
    "    print(f\"  {len(filtered)} after distance filter\")\n",
    "    \n",
    "    all_pair_data[f\"{frame_a}-{frame_b}\"] = {\n",
    "        'frame_a': frame_a,\n",
    "        'frame_b': frame_b,\n",
    "        'matches': filtered\n",
    "    }\n",
    "\n",
    "print(f\"\\nTotal pairs processed: {len(all_pair_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e201471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing pair 1-5\n",
      "  157 inliers\n",
      "\n",
      "Processing pair 1-10\n",
      "  53 inliers\n",
      "\n",
      "Processing pair 5-10\n",
      "  17 inliers\n",
      "\n",
      "Processing pair 5-15\n",
      "  9 inliers\n",
      "\n",
      "Processing pair 10-15\n",
      "  88 inliers\n",
      "\n",
      "Processing pair 10-20\n",
      "  18 inliers\n",
      "\n",
      "Processing pair 15-20\n",
      "  33 inliers\n",
      "\n",
      "RANSAC complete for 7 pairs\n"
     ]
    }
   ],
   "source": [
    "all_ransac_results = {}\n",
    "\n",
    "for pair_name, pair_info in all_pair_data.items():\n",
    "    print(f\"\\nProcessing pair {pair_name}\")\n",
    "    \n",
    "    filtered_matches = pair_info['matches']\n",
    "    \n",
    "    # Extract 2D points\n",
    "    pts_a = np.array([m['pt_a'] for m in filtered_matches])\n",
    "    pts_b = np.array([m['pt_b'] for m in filtered_matches])\n",
    "    \n",
    "    # Use OpenCV's findEssentialMat with RANSAC\n",
    "    E, mask = cv2.findEssentialMat(pts_a, pts_b, K, method=cv2.RANSAC, threshold=1.0)\n",
    "    \n",
    "    # Recover pose\n",
    "    _, R, t, mask_pose = cv2.recoverPose(E, pts_a, pts_b, K, mask=mask)\n",
    "    \n",
    "    # Extract inliers\n",
    "    inlier_indices = np.where(mask_pose.ravel() > 0)[0]\n",
    "    pts_a_inliers = pts_a[inlier_indices]\n",
    "    pts_b_inliers = pts_b[inlier_indices]\n",
    "    \n",
    "    # Build projection matrices\n",
    "    P_a = K @ np.hstack([np.eye(3), np.zeros((3, 1))])\n",
    "    P_b = K @ np.hstack([R, t])\n",
    "    \n",
    "    all_ransac_results[pair_name] = {\n",
    "        'frame_a': pair_info['frame_a'],\n",
    "        'frame_b': pair_info['frame_b'],\n",
    "        'pts_a': pts_a_inliers,\n",
    "        'pts_b': pts_b_inliers,\n",
    "        'P_a': P_a,\n",
    "        'P_b': P_b,\n",
    "        'R': R,\n",
    "        't': t,\n",
    "        'inlier_indices': inlier_indices,\n",
    "        'filtered_matches': filtered_matches\n",
    "    }\n",
    "    \n",
    "    print(f\"  {len(inlier_indices)} inliers\")\n",
    "\n",
    "print(f\"\\nRANSAC complete for {len(all_ransac_results)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf09350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Triangulating pair 1-5\n",
      "  157 valid 3D points\n",
      "\n",
      "Triangulating pair 1-10\n",
      "  53 valid 3D points\n",
      "\n",
      "Triangulating pair 5-10\n",
      "  17 valid 3D points\n",
      "\n",
      "Triangulating pair 5-15\n",
      "  9 valid 3D points\n",
      "\n",
      "Triangulating pair 10-15\n",
      "  88 valid 3D points\n",
      "\n",
      "Triangulating pair 10-20\n",
      "  18 valid 3D points\n",
      "\n",
      "Triangulating pair 15-20\n",
      "  33 valid 3D points\n",
      "\n",
      "Total 3D points in database: 375\n",
      "Feature database size: 375\n"
     ]
    }
   ],
   "source": [
    "all_triangulated = []\n",
    "\n",
    "for pair_name, pair_data in all_ransac_results.items():\n",
    "    print(f\"\\nTriangulating pair {pair_name}\")\n",
    "    \n",
    "    frame_a = pair_data['frame_a']\n",
    "    frame_b = pair_data['frame_b']\n",
    "    \n",
    "    # Triangulate\n",
    "    points_4d = cv2.triangulatePoints(\n",
    "        pair_data['P_a'], \n",
    "        pair_data['P_b'], \n",
    "        pair_data['pts_a'].T, \n",
    "        pair_data['pts_b'].T\n",
    "    )\n",
    "    points_3d = points_4d[:3] / points_4d[3]\n",
    "    points_3d = points_3d.T\n",
    "    \n",
    "    # Filter by positive depth\n",
    "    valid_points = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, pt_3d in enumerate(points_3d):\n",
    "        pt_homog = np.append(pt_3d, 1)\n",
    "        depth_a = (pair_data['P_a'] @ pt_homog)[2]\n",
    "        depth_b = (pair_data['P_b'] @ pt_homog)[2]\n",
    "        \n",
    "        if depth_a > 0 and depth_b > 0:\n",
    "            valid_points.append(pt_3d)\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    print(f\"  {len(valid_points)} valid 3D points\")\n",
    "    \n",
    "    # Add to database with descriptors from frame_a\n",
    "    for i in valid_indices:\n",
    "        inlier_idx = pair_data['inlier_indices'][i]\n",
    "        match = pair_data['filtered_matches'][inlier_idx]\n",
    "        \n",
    "        descriptor = features[frame_a]['descriptors'][match['kp_idx_a']]\n",
    "        pt_3d = valid_points[valid_indices.index(i)]\n",
    "        \n",
    "        all_triangulated.append({\n",
    "            'descriptor': descriptor,\n",
    "            'point_3d': pt_3d,\n",
    "            'from_pair': pair_name\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal 3D points in database: {len(all_triangulated)}\")\n",
    "\n",
    "# Remove duplicate 3D points (keep best descriptor)\n",
    "# For now, just use all points\n",
    "feature_database = all_triangulated\n",
    "print(f\"Feature database size: {len(feature_database)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc98d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 matches\n",
      "Unique correspondences: 27\n",
      "\n",
      "Estimated position (relative coords): [0.77659775 2.05069565 0.42963156]\n"
     ]
    }
   ],
   "source": [
    "# Query with frame 25\n",
    "query_descriptors = features[25]['descriptors']\n",
    "query_keypoints = features[25]['keypoints']\n",
    "\n",
    "db_descriptors = np.array([entry['descriptor'] for entry in feature_database])\n",
    "\n",
    "matches = extractor.match_features(query_descriptors, db_descriptors)\n",
    "print(f\"Found {len(matches)} matches\")\n",
    "\n",
    "# Extract 2D-3D correspondences and filter duplicates\n",
    "best_matches = {}\n",
    "for match in matches:\n",
    "    query_pt = query_keypoints[match.queryIdx].pt\n",
    "    world_pt = feature_database[match.trainIdx]['point_3d']\n",
    "    world_pt_tuple = tuple(world_pt)\n",
    "    \n",
    "    if world_pt_tuple not in best_matches or match.distance < best_matches[world_pt_tuple]['distance']:\n",
    "        best_matches[world_pt_tuple] = {\n",
    "            'query_2d': query_pt,\n",
    "            'world_3d': world_pt,\n",
    "            'distance': match.distance\n",
    "        }\n",
    "\n",
    "filtered_query_2d = np.array([entry['query_2d'] for entry in best_matches.values()])\n",
    "filtered_world_3d = np.array([entry['world_3d'] for entry in best_matches.values()])\n",
    "\n",
    "print(f\"Unique correspondences: {len(filtered_query_2d)}\")\n",
    "\n",
    "# Solve PnP\n",
    "if len(filtered_query_2d) >= 4:\n",
    "    success, rvec, tvec = cv2.solvePnP(\n",
    "        filtered_world_3d,\n",
    "        filtered_query_2d,\n",
    "        K,\n",
    "        None,\n",
    "        flags=cv2.SOLVEPNP_ITERATIVE\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        t_est = tvec.flatten()\n",
    "        print(f\"\\nEstimated position (relative coords): {t_est}\")\n",
    "        \n",
    "        # Store for alignment\n",
    "        estimated_pose = t_est\n",
    "    else:\n",
    "        print(\"PnP failed\")\n",
    "else:\n",
    "    print(f\"Not enough correspondences: {len(filtered_query_2d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d230d5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our camera positions (relative coords):\n",
      "  Frame 1: [0 0 0]\n",
      "  Frame 5: [ 0.30532549  0.23494159 -0.92281027]\n",
      "  Frame 10: [ 0.97675756 -0.08662906  0.19606141]\n",
      "\n",
      "Ground truth positions:\n",
      "  Frame 1: [ 4.         -0.06        1.79999995]\n",
      "  Frame 5: [3.92506528 0.77295786 1.79999995]\n",
      "  Frame 10: [3.59126544 1.7625016  1.79999995]\n",
      "  Frame 15: [3.01272702 2.63193393 1.79999995]\n",
      "  Frame 20: [2.22887635 3.32200384 1.79999995]\n",
      "  Frame 25: [1.29313135 3.78568506 1.79999995]\n"
     ]
    }
   ],
   "source": [
    "# Get camera positions in both coordinate systems\n",
    "# In our system: extract from the projection matrices we built\n",
    "our_camera_positions = {}\n",
    "\n",
    "# Frame 1 is always at origin\n",
    "our_camera_positions[1] = np.array([0, 0, 0])\n",
    "\n",
    "# For other frames, extract from their relative poses\n",
    "for pair_name, pair_data in all_ransac_results.items():\n",
    "    frame_a = pair_data['frame_a']\n",
    "    frame_b = pair_data['frame_b']\n",
    "    \n",
    "    if frame_a == 1:\n",
    "        # Camera position is -R^T @ t\n",
    "        cam_b_pos = -pair_data['R'].T @ pair_data['t'].flatten()\n",
    "        our_camera_positions[frame_b] = cam_b_pos\n",
    "\n",
    "print(\"Our camera positions (relative coords):\")\n",
    "for frame, pos in sorted(our_camera_positions.items()):\n",
    "    print(f\"  Frame {frame}: {pos}\")\n",
    "\n",
    "# Ground truth positions\n",
    "gt_camera_positions = {}\n",
    "for frame in [1, 5, 10, 15, 20, 25]:\n",
    "    gt_pose = ground_truth['poses'][frame-1]['left_camera']\n",
    "    gt_camera_positions[frame] = np.array(gt_pose['translation'])\n",
    "\n",
    "print(\"\\nGround truth positions:\")\n",
    "for frame, pos in sorted(gt_camera_positions.items()):\n",
    "    print(f\"  Frame {frame}: {pos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfeat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
